{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Q-Learning (Reinforcement Learning) based Maze Solver**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Required Libraries & Modules:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import numpy as np\r\n",
    "import random\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from ImgPreprocess import preprocess"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent of the Q-Learning Model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class Agent:\r\n",
    "    # defining Agent related functions and instances\r\n",
    "\r\n",
    "    def __init__(self, i = 0, j = 0):\r\n",
    "        # initailises the Agent object with passed coordinates (default: (0, 0))\r\n",
    "\r\n",
    "        self.i = i\r\n",
    "        self.j = j\r\n",
    "\r\n",
    "    @property\r\n",
    "    def loc(self):\r\n",
    "        # returns the current location of the Agent in the maze as tuple\r\n",
    "\r\n",
    "        return (self.i, self.j)\r\n",
    "    \r\n",
    "    def vmove(self, direction):\r\n",
    "        # returns a new object of the Agent after moving vertically \r\n",
    "\r\n",
    "        direction = 1 if direction > 0 else -1\r\n",
    "        return Agent(self.i + direction, self.j)\r\n",
    "    \r\n",
    "    def hmove(self, direction):\r\n",
    "        # returns a new object of the Agent after moving horizontally\r\n",
    "\r\n",
    "        direction = 1 if direction > 0 else -1\r\n",
    "        return Agent(self.i, direction + self.j) \r\n",
    "    \r\n",
    "    def __repr__(self):\r\n",
    "        # returns a string denoting the current location of the Agent in the maze\r\n",
    "        \r\n",
    "        return str(self.loc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defining the Maze: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class Maze:\r\n",
    "    # general maze class defination and related functions and instances\r\n",
    "\r\n",
    "    def __init__(self, rows = 4, columns = 4, l = [0, 0, -1, -1], aspect = 1, reward = 0):\r\n",
    "        # initializing a matrix of order (rows x columns) (default: 4x4) with 0s\r\n",
    "        # mousy is an object of Agent class \r\n",
    "        # mousy starts at cell (x0, y0) (default: top-left corner)\r\n",
    "        # end point is at cell (x1, y1) (default: bottom-right corner)\r\n",
    "        \r\n",
    "        self.r = rows\r\n",
    "        self.c = columns\r\n",
    "        if reward == 0: # reward is proportional to the size of the maze\r\n",
    "            self.reward = 10 * rows * columns \r\n",
    "        else: self.reward = reward # manually set\r\n",
    "        self.env = np.zeros((rows, columns))\r\n",
    "        self.x0, self.y0, self.x1, self.y1 = l\r\n",
    "        self.mousy = Agent(self.x0, self.y0) # defining the start cell\r\n",
    "\r\n",
    "        self.env[self.x1, self.y1] = 1 # defining the end cell\r\n",
    "        if self.x0 < 0: self.x0 = self.x0 % rows\r\n",
    "        if self.y0 < 0: self.y0 = self.y0 % columns\r\n",
    "        if self.x1 < 0: self.x1 = self.x1 % rows\r\n",
    "        if self.y1 < 0: self.y1 = self.y1 % columns\r\n",
    "            \r\n",
    "        self.trace = [] # for tracking path of agent\r\n",
    "        self.aspect = aspect # aspect ratio of Matplotlib plots (default: 1)\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        # resetting mousy to original coordinates (x0, y0) \r\n",
    "\r\n",
    "        self.mousy.i = self.x0\r\n",
    "        self.mousy.j = self.y0\r\n",
    "        self.trace = []\r\n",
    "\r\n",
    "    def state_for_agent(self):\r\n",
    "        # indexing each grid cell of maze as a unique state with unique index \r\n",
    "        # and returning the index to current position of mousy\r\n",
    "\r\n",
    "        a = self.mousy\r\n",
    "        return a.i * self.c + a.j\r\n",
    "        \r\n",
    "    def in_bounds(self, i, j):\r\n",
    "        # checks if the coordinates are outside the maze \r\n",
    "\r\n",
    "        return i >= 0 and i < self.r and j >= 0 and j < self.c\r\n",
    "    \r\n",
    "    def agent_in_bounds(self, a):\r\n",
    "        # checks if the agent is inside the maze or not\r\n",
    "\r\n",
    "        return self.in_bounds(a.i, a.j)\r\n",
    "    \r\n",
    "    def agent_dient(self, a):\r\n",
    "        # checks if the coordinates are not prohibited/ are not walls\r\n",
    "\r\n",
    "        return self.env[a.i, a.j] != -1\r\n",
    "    \r\n",
    "    def is_valid_new_agent(self, a):\r\n",
    "        # checks if the agent state is valid, i.e. niether outside maze nor on walls \r\n",
    "\r\n",
    "        return self.agent_in_bounds(a) and self.agent_dient(a)\r\n",
    "    \r\n",
    "    @property\r\n",
    "    def all_actions(self):  \r\n",
    "        # returns all the actions that mousy can take (both valid or invalid) as a \r\n",
    "        # list of objects of Agent class\r\n",
    "\r\n",
    "        a = self.mousy    \r\n",
    "        return [\r\n",
    "            a.vmove(1),  # down\r\n",
    "            a.vmove(-1), # up\r\n",
    "            a.hmove(1),  # right\r\n",
    "            a.hmove(-1), # left\r\n",
    "        ]\r\n",
    "       \r\n",
    "    def compute_possible_moves(self):\r\n",
    "        # returns all the valid actions that mousy can take as a list of tuples containing  \r\n",
    "        # object of Agent class and its index\r\n",
    "\r\n",
    "        moves = self.all_actions\r\n",
    "        return [(m, ii) for ii, m in enumerate(moves) if self.is_valid_new_agent(m)]\r\n",
    "\r\n",
    "    def has_won(self):\r\n",
    "        # returns if mousy has won (reached the end of the matrix)\r\n",
    "\r\n",
    "        a = self.mousy\r\n",
    "        return self.env[a.i, a.j] == 1\r\n",
    "            \r\n",
    "    def do_a_move(self,a):\r\n",
    "        # moves mousy to a new valid position on the maze, updates the trace and returns \r\n",
    "        # the corresponding score: 10 for winning else -0.1\r\n",
    "\r\n",
    "        if self.is_valid_new_agent(a): \r\n",
    "            self.trace.append(self.mousy.loc) # Update trace\r\n",
    "            self.mousy = a\r\n",
    "            return self.reward if self.has_won() else -0.1\r\n",
    "        else: \r\n",
    "            self.show_trace()\r\n",
    "            print(a.i, a.j)\r\n",
    "            assert False, \"Mousy can't go there\"\r\n",
    "        \r\n",
    "    def visualise(self):\r\n",
    "        # prints the entire maze with the agent's location\r\n",
    "        # 0's denote valid cells, -1's denote walls \r\n",
    "        # 1 denotes the end of the maze and 6 denotes the current position of agent\r\n",
    "\r\n",
    "        assert self.agent_in_bounds(self.mousy), \"mousy is out of bounds\"\r\n",
    "        ec = self.env.copy()\r\n",
    "        mo = self.mousy\r\n",
    "        ec[mo.i,mo.j] = 6\r\n",
    "        print(ec)\r\n",
    "    \r\n",
    "    def show_trace(self):\r\n",
    "        # shows a pictorial representation of path trace using Matplotlib\r\n",
    "        # aspect defines the height to width ratio of the pixels\r\n",
    "        \r\n",
    "        ec = self.env.copy()\r\n",
    "        mo = self.mousy\r\n",
    "        for x in self.trace: ec[x[0], x[1]] = -0.5\r\n",
    "        ec[mo.i, mo.j] = 0.5\r\n",
    "        plt.imshow(ec, aspect = self.aspect)\r\n",
    "        plt.clim(-1, 1)\r\n",
    "        plt.colorbar()\r\n",
    "        plt.show()\r\n",
    "        \r\n",
    "    def show_maze(self):\r\n",
    "        # shows a pictorial representation of the maze using Matplotlib\r\n",
    "        # aspect defines the height to width ratio of the pixels\r\n",
    "\r\n",
    "        ec = self.env.copy()\r\n",
    "        mo = self.mousy\r\n",
    "        ec[mo.i,mo.j] = 0.5\r\n",
    "        plt.imshow(ec, aspect = self.aspect)\r\n",
    "        plt.clim(-1, 1)\r\n",
    "        plt.colorbar()\r\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q-Learning Model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class Qlearning:\r\n",
    "    # implements the Reinforcement Learning (Q-Learning) algorithm using Bellman Equation\r\n",
    "\r\n",
    "    def __init__(self, num_states, num_actions, lr = 0.1, df = 0.99):\r\n",
    "        # initializing the learning rate(a) with lr(default: 0.1), the discount factor(g) \r\n",
    "        # with df(default: 0.99) and the q-matrix(q) of order (num_states x num_actions) \r\n",
    "        # with 0s\r\n",
    "        \r\n",
    "        self.q = np.zeros((num_states, num_actions))\r\n",
    "        self.a = lr\r\n",
    "        self.g = df\r\n",
    "    \r\n",
    "    def update(self, st, at, rt, st1):\r\n",
    "        # updates the q-matrix using Bellman Equation\r\n",
    "\r\n",
    "        q = self.q  \r\n",
    "        a = self.a\r\n",
    "        g = self.g\r\n",
    "        q[st, at] = (1-a) * q[st, at] + a * (rt + g * np.max(q[st1]))\r\n",
    "        \r\n",
    "    def __str__(self):\r\n",
    "        # Returns the q-matrix as string\r\n",
    "        \r\n",
    "        rows, cols = self.q.shape\r\n",
    "        s = \"\"\r\n",
    "        for r in range(rows):\r\n",
    "            for c in range(cols):\r\n",
    "                s += str(self.q[r, c]) + \", \"\r\n",
    "            s += '\\n'\r\n",
    "\r\n",
    "        return s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Maze Generator:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def make_test_maze(l, s = 4): \r\n",
    "    # returns a random s x s test maze which the algorithm solves using RL\r\n",
    "    # default size: 4x4\r\n",
    "\r\n",
    "    m = Maze(s, s, l)\r\n",
    "    e = m.env\r\n",
    "\r\n",
    "    for i in range(len(e)):\r\n",
    "        for j in range(len(e[i])):\r\n",
    "            # randomly converts about 35% of maze cells to walls \r\n",
    "            # except for start and end of the maze\r\n",
    "\r\n",
    "            if (i == m.x0 and j == m.y0) or (i == m.x1 and j == m.y1) : continue\r\n",
    "            if random.random() < 0.35:\r\n",
    "                e[i, j] = -1\r\n",
    "    return m"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def final(M, q):\r\n",
    "    print(\"----------------------BEST SOLUTION OF THE MAZE----------------------\")\r\n",
    "    M.reset()\r\n",
    "    M.show_maze()\r\n",
    "    while not M.has_won():\r\n",
    "        s = M.state_for_agent()\r\n",
    "        a_idx = np.argmax(q[s])\r\n",
    "        M.do_a_move(M.all_actions[a_idx])\r\n",
    "        M.show_trace()\r\n",
    "        \r\n",
    "    M.visualise()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Driver Code:\n",
    "* ### _Exploration_ :\n",
    "    Randomly travelling the maze and updating the q-matrix\n",
    "* ### _Exploitation_ :\n",
    "    Using the currently available q-matrix to travel along the path of maximum reward\n",
    "### Random maze generate:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def main1():\r\n",
    "    # DRIVER CODE\r\n",
    "\r\n",
    "    s = int(input(\"Enter the size of the maze:\"))\r\n",
    "    q = Qlearning(s**2, 4)\r\n",
    "\r\n",
    "    l = list(map(int, input(\"Enter space-separated coordinates of start and end points, Press 'd' for default values: \").split()))\r\n",
    "\r\n",
    "    if l[0] == 'd' : l = [0, 0, -1, -1]\r\n",
    "    \r\n",
    "    # Flag boolean value, remains 'False' until a solvable maze is generated\r\n",
    "    go_ahead = False \r\n",
    "    \r\n",
    "    # generating random mazes and manually checking if solvable\r\n",
    "    while not go_ahead:\r\n",
    "        M = make_test_maze(l, s)\r\n",
    "        M.show_maze()\r\n",
    "        ctu = input (\"Press 'n' to generate a new maze else hit 'Enter':\")\r\n",
    "        if ctu.lower() == 'n':\r\n",
    "            continue\r\n",
    "        go_ahead = True\r\n",
    "    \r\n",
    "    # running the algorithm for 500 episodes to generate a q-matrix\r\n",
    "    # to get the most efficient solution\r\n",
    "    for i in range(500):\r\n",
    "        # Maze reset\r\n",
    "        M.reset()  \r\n",
    "        final_score = 0\r\n",
    "        \r\n",
    "        itr = 0\r\n",
    "        while not M.has_won():\r\n",
    "            # iterating until mousy has won in each episode\r\n",
    "            # simulataneously updating the q-matrix\r\n",
    "            itr += 1\r\n",
    "            st = M.state_for_agent() # current state of mousy\r\n",
    "                   \r\n",
    "            if random.random() > 0.7  or i < 150:\r\n",
    "                # exploration: for the first 150 episodes and 30% of the remaining \r\n",
    "                # 350 on average\r\n",
    "                moves = M.compute_possible_moves()\r\n",
    "                random.shuffle(moves) # randomly choosing from the available moves\r\n",
    "                move, move_idx = moves[0]\r\n",
    "\r\n",
    "            else:\r\n",
    "                # exploitation: for 70% of the remaining episodes after first 50 \r\n",
    "                # on average\r\n",
    "                moves = M.all_actions\r\n",
    "                \r\n",
    "                # sorting the q-values in descending order\r\n",
    "                q_val = q.q[st] \r\n",
    "                y = [x for x in enumerate(q_val)]\r\n",
    "                y.sort(key = lambda p: p[1], reverse = True) \r\n",
    "                                \r\n",
    "                # choosing valid new move with maximum reward\r\n",
    "                for z in y:\r\n",
    "                    move_idx = z[0]\r\n",
    "                    move = moves[move_idx]\r\n",
    "                    if M.is_valid_new_agent(move): break\r\n",
    "\r\n",
    "            at = move_idx\r\n",
    "            score = M.do_a_move(move)                     \r\n",
    "            final_score += score \r\n",
    "            rt = score\r\n",
    "            st1 = M.state_for_agent()\r\n",
    "\r\n",
    "            q.update(st, at , rt, st1) # updating the q-matrix after a move\r\n",
    "        \r\n",
    "        print(f\"finished episode {i} with a final score of {final_score} and in {itr} iterations\")\r\n",
    "        print(\"END\")\r\n",
    "\r\n",
    "    print(\"----------------------Q-MATRIX----------------------\\n\", q) # the final q-matrix\r\n",
    "                \r\n",
    "    # printing the most efficient solution\r\n",
    "    final(M, q.q)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Image input maze generate:\n",
    "\n",
    "'ep_part' ought to be an iterable like tuple or list.\n",
    "* the first element defines the total number of episodes\n",
    "* the second element defines the initial number of episodes for which the agent only explores the maze\n",
    "* the third element is a fraction, defining the part of the remaining episodes for which the agent exploits the maze"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def main2(s, margin = 0.01, way = None, pix = 1, div = 128, aspect = 1, ep_part = [500, 150, 0.7], reward = 0):\r\n",
    "    # DRIVER CODE\r\n",
    "    \r\n",
    "    p = preprocess(s)\r\n",
    "    p.generate(margin, way, pix, div)\r\n",
    "    q = Qlearning(p.h * p.w, 4)\r\n",
    "    M = Maze(p.h, p.w, p.loc, aspect)\r\n",
    "    M.env = p.nim\r\n",
    "    M.show_maze()\r\n",
    "    \r\n",
    "    # running the algorithm for specified number of episodes to generate \r\n",
    "    # a q-matrix to get the most efficient solution\r\n",
    "    for i in range(ep_part[0]):\r\n",
    "        # Maze reset\r\n",
    "        M.reset()  \r\n",
    "        final_score = 0\r\n",
    "        \r\n",
    "        itr = 0\r\n",
    "        while not M.has_won():\r\n",
    "            # iterating until mousy has won in each episode\r\n",
    "            # simulataneously updating the q-matrix\r\n",
    "            itr += 1\r\n",
    "            st = M.state_for_agent() # current state of mousy\r\n",
    "                   \r\n",
    "            if random.random() > ep_part[2]  or i < ep_part[1]:\r\n",
    "                # exploration\r\n",
    "                moves = M.compute_possible_moves()\r\n",
    "                random.shuffle(moves) # randomly choosing from the available moves\r\n",
    "                move, move_idx = moves[0]\r\n",
    "\r\n",
    "            else:\r\n",
    "                # exploitation\r\n",
    "                moves = M.all_actions\r\n",
    "                \r\n",
    "                # sorting the q-values in descending order\r\n",
    "                q_val = q.q[st]\r\n",
    "                y = [x for x in enumerate(q_val)]\r\n",
    "                y.sort(key = lambda p: p[1], reverse = True) \r\n",
    "                                \r\n",
    "                # choosing valid new move with maximum reward\r\n",
    "                for z in y:\r\n",
    "                    move_idx = z[0]\r\n",
    "                    move = moves[move_idx]\r\n",
    "                    if M.is_valid_new_agent(move): break\r\n",
    "\r\n",
    "            at = move_idx\r\n",
    "            score = M.do_a_move(move)                     \r\n",
    "            final_score += score \r\n",
    "            rt = score\r\n",
    "            st1 = M.state_for_agent()\r\n",
    "\r\n",
    "            q.update(st, at , rt, st1) # updating the q-matrix after a move\r\n",
    "        \r\n",
    "        print(f\"finished episode {i} with a final score of {final_score} and in {itr} iterations\")\r\n",
    "        print(\"END\")\r\n",
    "\r\n",
    "    print(\"----------------------Q-MATRIX----------------------\\n\", q) # the final q-matrix\r\n",
    "                \r\n",
    "    # printing the most efficient solution\r\n",
    "    final(M, q.q)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "if __name__ == \"__main__\" :\r\n",
    "    main2(\"F:/New folder/meme/10.png\", margin = 0, pix = 0, ep_part = [100, 30, 0.6])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-f98a6997d7c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"F:/New folder/New folder/maze (1).png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmargin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-8ce3b176783d>\u001b[0m in \u001b[0;36mmain2\u001b[1;34m(s, margin, way, pix, div, aspect, ep_part, reward)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mst1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_for_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mat\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mrt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mst1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# updating the q-matrix after a move\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"finished episode {i} with a final score of {final_score} and in {itr} iterations\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-45ff88544a4e>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, st, at, rt, st1)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mst1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd4c67ef142469da7dc4d338a32ac40116904d26076b8e6aa587d80720bc6a2b"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}